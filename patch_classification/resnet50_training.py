from classes.MetricsTracker import MetricsTracker
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
from torch.utils.tensorboard import SummaryWriter
import argparse
from tqdm import tqdm
import io
from PIL import Image
import datetime
import pytz
import timm
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split

def parse_args():
    parser = argparse.ArgumentParser(description='ResNet50 Training Script')
    parser.add_argument('--gpu', type=int, default=1, 
                        help='GPU device index to use (0, 1, 2, 3). Default: 1')
    parser.add_argument('--epochs', type=int, default=15, help='Number of epochs')
    parser.add_argument('--patience', type=int, default=5, help='Early stopping patience')
    parser.add_argument('--lr', type=float, default=3e-4, help='Learning rate')
    parser.add_argument('--batch_size', type=int, default=64, help='Batch size')
    return parser.parse_args()

def setup_device(gpu_id):
    if not torch.cuda.is_available():
        print("CUDA not available! Using CPU.")
        return torch.device('cpu')
    
    num_gpus = torch.cuda.device_count()
    print(f"Available GPUs: {num_gpus}")
    
    if gpu_id >= num_gpus:
        print(f"Warning: GPU {gpu_id} not available. Using GPU 0 instead.")
        gpu_id = 0
    
    device = torch.device(f'cuda:{gpu_id}')
    torch.cuda.set_device(gpu_id)
    torch.cuda.empty_cache()
    
    print(f"Using GPU {gpu_id}: {torch.cuda.get_device_name(gpu_id)}")
    return device

def create_pretrained_model(device, num_classes=5, dropout_rate=0.3):
    model = timm.create_model(
        model_name="hf-hub:1aurent/resnet50.tcga_brca_simclr",
        pretrained=True,
        )
    
    # –†–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    num_features = 2048
    
    # –ó–∞–º–æ—Ä–æ–∑–∫–∞ –≤–µ—Å–æ–≤
    for param in model.parameters():
        param.requires_grad = False

    if hasattr(model, 'layer4'):
        for param in model.layer4.parameters():
            param.requires_grad = True
    
    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞
    model.fc = nn.Sequential(
        nn.Linear(num_features, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Dropout(dropout_rate),
        nn.Linear(512, num_classes)
    )
    
    return model.to(device)

def simple_early_stopping(val_f1_macro, best_val_f1, patience_counter, patience=7):
    """–ü—Ä–æ—Å—Ç–æ–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π early stopping"""
    if val_f1_macro > best_val_f1 + 0.001:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
        return val_f1_macro, 0, True  # new_best, new_patience, save_model
    else:
        return best_val_f1, patience_counter + 1, False

def log_confusion_matrix_to_tensorboard(writer, cm, class_names, epoch, phase):
    """–õ–æ–≥–∏—Ä—É–µ–º confusion matrix –≤ TensorBoard"""
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm_norm, annot=True, fmt='.3f', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names, ax=ax)
    ax.set_title(f'Normalized Confusion Matrix - {phase} (Epoch {epoch+1})')
    ax.set_ylabel('True Label')
    ax.set_xlabel('Predicted Label')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±—É—Ñ–µ—Ä
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', dpi=150)
    buf.seek(0)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
    image = Image.open(buf)
    img_array = np.array(image)
    
    if img_array.shape[2] == 4:
        img_array = img_array[:, :, :3]
    
    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)
    writer.add_image(f'ConfusionMatrix/{phase}', img_tensor, epoch)
    
    plt.close(fig)
    buf.close()

def train_epoch(device, model, generator, criterion, optimizer, metrics_tracker, epoch, writer):
    model.train()
    metrics_tracker.reset()
    
    progress_bar = tqdm(generator, desc=f"Training Epoch {epoch+1}")
    
    for _, (features, labels) in enumerate(progress_bar):
        features = features.to(device).float()
        labels = labels.to(device).long()
        
        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        
        # Backward pass —Å gradient clipping
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        _, preds = torch.max(outputs, 1)
        metrics_tracker.update(preds, labels, loss.item())
        
        # –û–±–Ω–æ–≤–ª—è–µ–º progress bar
        progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})
    
    return metrics_tracker.compute_metrics()

def validate_model(device, model, val_generator, criterion, metrics_tracker, epoch):
    model.eval()
    metrics_tracker.reset()
    
    progress_bar = tqdm(val_generator, desc=f"Validation Epoch {epoch+1}")
    
    with torch.no_grad():
        for features, labels in progress_bar:
            features = features.to(device).float()
            labels = labels.to(device).long()
            
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            _, preds = torch.max(outputs, 1)
            metrics_tracker.update(preds, labels, loss.item())
            
            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})
    
    return metrics_tracker.compute_metrics()

def get_train_transforms():
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π"""
    return transforms.Compose([
        transforms.Resize((256, 256)),  # –Ω–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ —á–µ–º —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä
        transforms.RandomCrop(224),     # —Å–ª—É—á–∞–π–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –¥–æ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),    # –¥–ª—è –≥–∏—Å—Ç–æ–ª–æ–≥–∏–∏ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ —Ñ–ª–∏–ø—ã —Ç–æ–∂–µ –æ–∫
        transforms.RandomRotation(degrees=90),    # –ø–æ–≤–æ—Ä–æ—Ç –Ω–∞ 0, 90, 180, 270 –≥—Ä–∞–¥—É—Å–æ–≤
        transforms.ColorJitter(
            brightness=0.2,    # –∏–∑–º–µ–Ω–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏
            contrast=0.2,      # –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
            saturation=0.1,    # –Ω–µ–±–æ–ª—å—à–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏
            hue=0.05          # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Ç—Ç–µ–Ω–∫–∞
        ),
        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])

def get_test_transforms():
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])

def main():
    test_data_path = "/home/o.shtykova/interactive_segmentation/patches/patches_train_layer2"
    args = parse_args()
    device = setup_device(args.gpu)
    print(f"Using device: {device}")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    EPOCHS = args.epochs
    LEARNING_RATE = args.lr
    BATCH_SIZE = args.batch_size
    WEIGHT_DECAY = 1e-4
    PATIENCE = args.patience
    MIN_DELTA = 0.005  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–ª—è early stopping

    print(f"Training parameters:")
    print(f"  Epochs: {EPOCHS}")
    print(f"  Learning Rate: {LEARNING_RATE}")
    print(f"  Batch Size: {BATCH_SIZE}")
    print(f"  Patience: {PATIENCE}")
    print(f"  Min Delta: {MIN_DELTA}")

    # –ö–ª–∞—Å—Å—ã –∏ –∏—Ö –≤–µ—Å–∞ (–ø–µ—Ä–µ—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –≤–µ—Å–∞)
    class_names = ['AT', 'BG', 'LP', 'MM', 'TUM']

    model = create_pretrained_model(device, num_classes=5, dropout_rate=0.3)
    
    criterion = nn.CrossEntropyLoss()

    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=LEARNING_RATE, 
        weight_decay=WEIGHT_DECAY,
        eps=1e-8
    )
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-8
    )

    # –¢—Ä–µ–∫–µ—Ä—ã –º–µ—Ç—Ä–∏–∫
    train_metrics = MetricsTracker(class_names)
    val_metrics = MetricsTracker(class_names)

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    history = {
        'train_loss': [], 'train_acc': [], 'train_f1': [], 'train_min_f1': [],
        'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_min_f1': [],
        'train_f1_per_class': [], 'val_f1_per_class': []
    }

    # –°–æ–∑–¥–∞–µ–º –¥–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è–º–∏
    full_dataset_train_transforms = ImageFolder(test_data_path, transform=get_train_transforms())
    full_dataset_test_transforms = ImageFolder(test_data_path, transform=get_test_transforms())

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
    train_len = int(0.9 * len(full_dataset_train_transforms))
    val_len = len(full_dataset_train_transforms) - train_len

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±–æ–∏—Ö —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–π
    generator = torch.Generator().manual_seed(42)
    train_dataset, _ = random_split(full_dataset_train_transforms, [train_len, val_len], generator=generator)

    generator = torch.Generator().manual_seed(42)  # —Ç–æ—Ç –∂–µ seed!
    _, val_dataset = random_split(full_dataset_test_transforms, [train_len, val_len], generator=generator)

    # –°–æ–∑–¥–∞–µ–º DataLoader'—ã
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    save_dir = Path(f'exps/pretrained_resnet50_training_{datetime.datetime.now(pytz.timezone('Europe/Moscow')).strftime("%d-%m-%y_%H:%M")}')
    save_dir.mkdir(exist_ok=True)
    log_dir = save_dir / 'tensorboard_logs'
    log_dir.mkdir(exist_ok=True)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º TensorBoard writer
    writer = SummaryWriter(log_dir)

    print("\nStarting improved training...")
    print("=" * 80)
    print(f"TensorBoard logs: {log_dir}")
    print("Run: tensorboard --logdir improved_resnet50_training/tensorboard_logs")
    print("=" * 80)

    best_val_f1 = 0.0
    best_min_f1 = 0.0  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π F1 –ø–æ –∫–ª–∞—Å—Å–∞–º
    patience_counter = 0
    
    # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    no_improvement_epochs = 0

    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    val_results = validate_model(device, model, val_loader, criterion, val_metrics, 0)
    
    print(f"Val   - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}")
    print(f"        Macro F1: {val_results['macro_f1']:.4f}, Min F1: {val_results['min_f1']:.4f}")

    # F1-scores –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    print(f"Val F1-scores –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for class_name in class_names:
        f1 = val_results['f1_per_class'].get(class_name, 0)
        print(f"  {class_name}: {f1:.4f}")

    for epoch in range(EPOCHS):
        start_time = time.time()
        print(f"\nEpoch {epoch+1}/{EPOCHS}")
        print("-" * 50)
        
        # –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        train_results = train_epoch(
            device, model, train_loader, criterion, optimizer, 
            train_metrics, epoch, writer
        )
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_results = validate_model(device, model, val_loader, criterion, val_metrics, epoch)
        
        # Scheduler step –ø–æ macro F1
        scheduler.step(val_results['macro_f1'])
        current_lr = optimizer.param_groups[0]['lr']
        
        # –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard
        writer.add_scalars('Loss', {
            'Train': train_results['loss'],
            'Val': val_results['loss']
        }, epoch)
        
        writer.add_scalars('Accuracy', {
            'Train': train_results['accuracy'],
            'Val': val_results['accuracy']
        }, epoch)
        
        writer.add_scalars('Macro_F1', {
            'Train': train_results['macro_f1'],
            'Val': val_results['macro_f1']
        }, epoch)
        
        writer.add_scalars('Min_F1', {
            'Train': train_results['min_f1'],
            'Val': val_results['min_f1']
        }, epoch)
        
        writer.add_scalar('Learning_Rate', current_lr, epoch)
        
        # F1-scores –ø–æ –∫–ª–∞—Å—Å–∞–º
        for class_name in class_names:
            writer.add_scalars(f'F1_Score/{class_name}', {
                'Train': train_results['f1_per_class'].get(class_name, 0),
                'Val': val_results['f1_per_class'].get(class_name, 0)
            }, epoch)
        
        # –õ–æ–≥–∏—Ä—É–µ–º confusion matrices
        if epoch % 3 == 0:  # –ö–∞–∂–¥—ã–µ 3 —ç–ø–æ—Ö–∏
            log_confusion_matrix_to_tensorboard(writer, train_results['confusion_matrix'], 
                                            class_names, epoch, 'Train')
            log_confusion_matrix_to_tensorboard(writer, val_results['confusion_matrix'], 
                                            class_names, epoch, 'Val')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        history['train_loss'].append(train_results['loss'])
        history['train_acc'].append(train_results['accuracy'])
        history['train_f1'].append(train_results['macro_f1'])
        history['train_min_f1'].append(train_results['min_f1'])
        history['val_loss'].append(val_results['loss'])
        history['val_acc'].append(val_results['accuracy'])
        history['val_f1'].append(val_results['macro_f1'])
        history['val_min_f1'].append(val_results['min_f1'])
        history['train_f1_per_class'].append(train_results['f1_per_class'])
        history['val_f1_per_class'].append(val_results['f1_per_class'])
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        epoch_time = time.time() - start_time
        
        print(f"\nTime: {epoch_time:.1f}s | LR: {current_lr:.2e}")
        print(f"Train - Loss: {train_results['loss']:.4f}, Acc: {train_results['accuracy']:.4f}")
        print(f"        Macro F1: {train_results['macro_f1']:.4f}, Min F1: {train_results['min_f1']:.4f}")
        print(f"Val   - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}")
        print(f"        Macro F1: {val_results['macro_f1']:.4f}, Min F1: {val_results['min_f1']:.4f}")
        
        # F1-scores –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        print(f"\nTrain F1-scores:")
        for class_name in class_names:
            f1 = train_results['f1_per_class'].get(class_name, 0)
            print(f"  {class_name}: {f1:.4f}")
        
        # F1-scores –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        print(f"Val F1-scores:")
        for class_name in class_names:
            f1 = val_results['f1_per_class'].get(class_name, 0)
            print(f"  {class_name}: {f1:.4f}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∏–∑–∫–∏–µ F1-scores
        low_f1_classes = [name for name, f1 in val_results['f1_per_class'].items() if f1 < 0.3]
        if low_f1_classes:
            print(f"‚ö†Ô∏è  Classes with low F1 (<0.3): {low_f1_classes}")
        
        current_metric = val_results['macro_f1']
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏:
        # 1) –£–ª—É—á—à–∏–ª—Å—è macro F1 –ò –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π F1 –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
        # 2) –ò–õ–ò –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π F1
        save_model = False

        save_reason = f"Better macro F1: {current_metric:.4f} (min F1: {val_results['min_f1']:.4f})"
        
        best_val_f1, patience_counter, save_model = simple_early_stopping(
            val_results['macro_f1'], best_val_f1, patience_counter
        )

        if save_model:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_f1': best_val_f1,
                'best_min_f1': best_min_f1,
                'val_f1_per_class': val_results['f1_per_class'],
                'class_names': class_names,
                'config': {
                    'learning_rate': LEARNING_RATE,
                    'batch_size': BATCH_SIZE,
                    'dropout_rate': 0.3
                }
            }, save_dir / 'best_model.pth')
            print(f"‚úÖ Model saved! {save_reason}")
        
        print(f"Patience: {patience_counter}/{PATIENCE}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è early stopping
        if patience_counter >= PATIENCE:
            print(f"\nüõë Early stopping: no improvement for {PATIENCE} epochs")
            break
            
        # –ï—Å–ª–∏ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –æ—á–µ–Ω—å –¥–æ–ª–≥–æ - —Ç–æ–∂–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
        if no_improvement_epochs >= PATIENCE * 2:
            print(f"\nüõë Training stopped: no significant improvement for {no_improvement_epochs} epochs")
            break
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        if (train_results['macro_f1'] - val_results['macro_f1']) > 0.15:
            print(f"‚ö†Ô∏è  Potential overfitting detected! Train F1: {train_results['macro_f1']:.4f}, Val F1: {val_results['macro_f1']:.4f}")

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º TensorBoard writer
    writer.close()

    print("\n" + "=" * 80)
    print("üéâ TRAINING COMPLETED!")
    print("=" * 80)
    print(f"üìä Best validation Macro F1: {best_val_f1:.4f}")
    print(f"üìä Best minimum F1 (worst class): {best_min_f1:.4f}")
    print(f"üìÅ All files saved to: {save_dir}")
    print(f"üìà TensorBoard logs: {log_dir}")
    print("\nüöÄ To view results:")
    print(f"   tensorboard --logdir {log_dir}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏ –∏—Å—Ç–æ—Ä–∏—é
    torch.save({
        'model_state_dict': model.state_dict(),
        'history': history,
        'class_names': class_names,
        'final_epoch': epoch,
        'best_metrics': {
            'best_val_f1': best_val_f1,
            'best_min_f1': best_min_f1
        }
    }, save_dir / 'final_model.pth')

    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 50)
    print("üìã FINAL TRAINING REPORT")
    print("=" * 50)
    
    if len(history['val_f1_per_class']) > 0:
        last_val_f1 = history['val_f1_per_class'][-1]
        print("Final F1-scores per class:")
        for class_name, f1 in last_val_f1.items():
            status = "‚úÖ" if f1 > 0.8 else "‚ö†Ô∏è" if f1 > 0.6 else "‚ùå"
            print(f"  {status} {class_name}: {f1:.4f}")
    
    print(f"\nFiles created:")
    print(f"  üìÑ best_model.pth - Best model checkpoint")
    print(f"  üìÑ final_model.pth - Final model + training history")
    print(f"  üìÅ tensorboard_logs/ - TensorBoard visualization")
    
    print(f"\nüí° Recommendations for testing:")
    if best_min_f1 < 0.6:
        print(f"  ‚ö†Ô∏è  Minimum F1 is low ({best_min_f1:.4f}). Consider:")
        print(f"      - Collecting more data for weak classes")
        print(f"      - Adjusting class weights")
        print(f"      - Using focal loss")
    if best_val_f1 > 0.85:
        print(f"  ‚úÖ Good macro F1! Model should perform well on test data.")
    else:
        print(f"  üìà Consider longer training or hyperparameter tuning.")

if __name__ == '__main__':
    main()